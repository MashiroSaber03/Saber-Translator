import logging
import time
import requests
import json
import re
import os
import sys
from pathlib import Path
from openai import OpenAI
from src.shared.openai_helpers import create_openai_client

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
root_dir = str(Path(__file__).resolve().parent.parent.parent)
if root_dir not in sys.path:
    sys.path.insert(0, root_dir)

# å¯¼å…¥é¡¹ç›®å†…æ¨¡å—
from src.shared import constants
from src.interfaces.baidu_translate_interface import BaiduTranslateInterface
from src.interfaces.youdao_translate_interface import YoudaoTranslateInterface

# å…¨å±€APIå®ä¾‹ç¼“å­˜
baidu_translate = BaiduTranslateInterface()
youdao_translate = YoudaoTranslateInterface()

logger = logging.getLogger("CoreTranslation")
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# --- è‡ªå®šä¹‰å¼‚å¸¸ ---
class TranslationParseException(Exception):
    """æ‰¹é‡ç¿»è¯‘å“åº”è§£æå¤±è´¥å¼‚å¸¸ï¼Œè§¦å‘é‡è¯•"""
    pass

# --- rpm Limiting Globals for Translation ---
_translation_rpm_last_reset_time_container = [0]
_translation_rpm_request_count_container = [0]
# ------------------------------------------

def _enforce_rpm_limit(rpm_limit: int, service_name: str, last_reset_time_ref: list, request_count_ref: list):
    """
    æ‰§è¡Œrpmï¼ˆæ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼‰é™åˆ¶æ£€æŸ¥å’Œç­‰å¾…ã€‚
    ä½¿ç”¨åˆ—è¡¨ä½œä¸ºå¼•ç”¨ç±»å‹æ¥ä¿®æ”¹å¤–éƒ¨çš„ last_reset_time å’Œ request_countã€‚

    Args:
        rpm_limit (int): æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°ã€‚å¦‚æœä¸º0æˆ–è´Ÿæ•°ï¼Œåˆ™ä¸é™åˆ¶ã€‚
        service_name (str): æœåŠ¡åç§°ï¼Œç”¨äºæ—¥å¿—è®°å½•ã€‚
        last_reset_time_ref (list): åŒ…å«ä¸Šæ¬¡é‡ç½®æ—¶é—´çš„åˆ—è¡¨ (e.g., [timestamp])ã€‚
        request_count_ref (list): åŒ…å«å½“å‰è¯·æ±‚è®¡æ•°çš„åˆ—è¡¨ (e.g., [count])ã€‚
    """
    if rpm_limit <= 0:
        return # æ— é™åˆ¶

    current_time = time.time()

    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®çª—å£
    if current_time - last_reset_time_ref[0] >= 60:
        logger.info(f"rpm: {service_name} - 1åˆ†é’Ÿçª—å£å·²è¿‡ï¼Œé‡ç½®è®¡æ•°å™¨å’Œæ—¶é—´ã€‚")
        last_reset_time_ref[0] = current_time
        request_count_ref[0] = 0

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°rpmé™åˆ¶
    if request_count_ref[0] >= rpm_limit:
        time_to_wait = 60 - (current_time - last_reset_time_ref[0])
        if time_to_wait > 0:
            logger.info(f"rpm: {service_name} - å·²è¾¾åˆ°æ¯åˆ†é’Ÿ {rpm_limit} æ¬¡è¯·æ±‚ä¸Šé™ã€‚å°†ç­‰å¾… {time_to_wait:.2f} ç§’...")
            time.sleep(time_to_wait)
            # ç­‰å¾…ç»“æŸåï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„çª—å£
            last_reset_time_ref[0] = time.time() # æ›´æ–°ä¸ºå½“å‰æ—¶é—´
            request_count_ref[0] = 0
        else:
            # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œå› ä¸ºä¸Šé¢çš„çª—å£é‡ç½®é€»è¾‘ä¼šå¤„ç†
            logger.info(f"rpm: {service_name} - çª—å£å·²è¿‡ä½†è®¡æ•°æœªé‡ç½®ï¼Œç«‹å³é‡ç½®ã€‚")
            last_reset_time_ref[0] = current_time
            request_count_ref[0] = 0
    
    # å¦‚æœæ˜¯çª—å£å†…çš„ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼Œè®¾ç½®çª—å£å¼€å§‹æ—¶é—´
    if request_count_ref[0] == 0 and last_reset_time_ref[0] == 0: # æˆ–è€… last_reset_time_ref[0] è¿œæ—©äº current_time - 60
        last_reset_time_ref[0] = current_time
        logger.info(f"rpm: {service_name} - å¯åŠ¨æ–°çš„1åˆ†é’Ÿè¯·æ±‚çª—å£ã€‚")

    request_count_ref[0] += 1
    logger.debug(f"rpm: {service_name} - å½“å‰çª—å£è¯·æ±‚è®¡æ•°: {request_count_ref[0]}/{rpm_limit if rpm_limit > 0 else 'æ— é™åˆ¶'}")

def _safely_extract_from_json(json_str, field_name):
    """
    å®‰å…¨åœ°ä»JSONå­—ç¬¦ä¸²ä¸­æå–ç‰¹å®šå­—æ®µï¼Œå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µã€‚

    Args:
        json_str (str): JSONæ ¼å¼çš„å­—ç¬¦ä¸²
        field_name (str): è¦æå–çš„å­—æ®µå

    Returns:
        str: æå–çš„æ–‡æœ¬ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›ç®€åŒ–å¤„ç†çš„åŸå§‹æ–‡æœ¬
    """
    # å°è¯•ç›´æ¥è§£æ
    try:
        data = json.loads(json_str)
        if field_name in data:
            return data[field_name]
    except (json.JSONDecodeError, TypeError, KeyError):
        pass

    # è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–
    try:
        # åŒ¹é… "field_name": "å†…å®¹" æˆ– "field_name":"å†…å®¹" çš„æ¨¡å¼
        pattern = r'"' + re.escape(field_name) + r'"\s*:\s*"(.+?)"'
        # å¤šè¡Œæ¨¡å¼ï¼Œä½¿ç”¨DOTALL
        match = re.search(pattern, json_str, re.DOTALL)
        if match:
            # åè½¬ä¹‰æå–çš„æ–‡æœ¬
            extracted = match.group(1)
            # å¤„ç†è½¬ä¹‰å­—ç¬¦
            extracted = extracted.replace('\\"', '"').replace('\\n', '\n').replace('\\\\', '\\')
            return extracted
    except Exception:
        pass

    # å¦‚æœä¾ç„¶å¤±è´¥ï¼Œå°è¯•æ¸…ç†æ˜æ˜¾çš„JSONç»“æ„ï¼Œä»…ä¿ç•™æ–‡æœ¬å†…å®¹
    try:
        # åˆ é™¤å¸¸è§JSONç»“æ„å­—ç¬¦
        cleaned = re.sub(r'[{}"\[\]]', '', json_str)
        # åˆ é™¤å­—æ®µåå’Œå†’å·
        cleaned = re.sub(fr'{field_name}\s*:', '', cleaned)
        # åˆ é™¤å¤šä½™ç©ºç™½
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned
    except Exception:
        # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        return json_str

def translate_single_text(text, target_language, model_provider, 
                          api_key=None, model_name=None, prompt_content=None, 
                          use_json_format=False, custom_base_url=None,
                          rpm_limit_translation: int = constants.DEFAULT_rpm_TRANSLATION,
                          max_retries: int = constants.DEFAULT_TRANSLATION_MAX_RETRIES):
    """
    ä½¿ç”¨æŒ‡å®šçš„å¤§æ¨¡å‹ç¿»è¯‘å•æ®µæ–‡æœ¬ã€‚
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°ç”¨äºé LLM æä¾›å•†ï¼ˆå¦‚ç™¾åº¦ç¿»è¯‘ï¼‰å’Œç¼–è¾‘æ¨¡å¼çš„å•æ°”æ³¡é‡ç¿»è¯‘ã€‚
    æ‰¹é‡ç¿»è¯‘è¯·ä½¿ç”¨ translate_text_list() å‡½æ•°ã€‚

    Args:
        text (str): éœ€è¦ç¿»è¯‘çš„åŸå§‹æ–‡æœ¬ã€‚
        target_language (str): ç›®æ ‡è¯­è¨€ä»£ç  (ä¾‹å¦‚ 'zh')ã€‚
        model_provider (str): æ¨¡å‹æä¾›å•†ã€‚
        api_key (str, optional): API å¯†é’¥ (å¯¹äºéæœ¬åœ°éƒ¨ç½²æ˜¯å¿…éœ€çš„)ã€‚
        model_name (str, optional): æ¨¡å‹åç§°ã€‚
        prompt_content (str, optional): è‡ªå®šä¹‰æç¤ºè¯ã€‚å¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯ã€‚
        use_json_format (bool): æ˜¯å¦æœŸæœ›å¹¶è§£æ JSON æ ¼å¼çš„å“åº”ã€‚
        custom_base_url (str, optional): ç”¨æˆ·è‡ªå®šä¹‰çš„ OpenAI å…¼å®¹ API çš„ Base URLã€‚
        rpm_limit_translation (int): ç¿»è¯‘æœåŠ¡çš„æ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶ã€‚
        max_retries (int): ç¿»è¯‘å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚
    Returns:
        str: ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› "ç¿»è¯‘å¤±è´¥: [åŸå› ]"ã€‚
    """
    if not text or not text.strip():
        return ""

    if prompt_content is None:
        # æ ¹æ®æ˜¯å¦ä½¿ç”¨ JSON æ ¼å¼é€‰æ‹©é»˜è®¤æç¤ºè¯
        if use_json_format:
            prompt_content = constants.DEFAULT_TRANSLATE_JSON_PROMPT
        else:
            prompt_content = constants.DEFAULT_PROMPT
    elif use_json_format and '"translated_text"' not in prompt_content:
        # å¦‚æœç”¨æˆ·ä¼ å…¥äº†è‡ªå®šä¹‰æç¤ºè¯ä½†ä¸æ˜¯JSONæ ¼å¼ï¼Œç»™å‡ºè­¦å‘Š
        logger.warning("æœŸæœ›JSONæ ¼å¼è¾“å‡ºï¼Œä½†æä¾›çš„ç¿»è¯‘æç¤ºè¯å¯èƒ½ä¸æ˜¯JSONæ ¼å¼ã€‚")


    logger.info(f"å¼€å§‹ç¿»è¯‘æ–‡æœ¬: '{text[:30]}...' (æœåŠ¡å•†: {model_provider}, rpm: {rpm_limit_translation if rpm_limit_translation > 0 else 'æ— '}, é‡è¯•: {max_retries})")

    retry_count = 0
    translated_text = "ã€ç¿»è¯‘å¤±è´¥ã€‘è¯·æ£€æŸ¥ç»ˆç«¯ä¸­çš„é”™è¯¯æ—¥å¿—"

    # --- rpm Enforcement ---
    # ä½¿ç”¨å®¹å™¨æ¥ä¼ é€’å¼•ç”¨
    _enforce_rpm_limit(
        rpm_limit_translation,
        f"Translation ({model_provider})",
        _translation_rpm_last_reset_time_container,
        _translation_rpm_request_count_container
    )
    # ---------------------

    while retry_count < max_retries:
        try:
            if model_provider == 'siliconflow':
                # SiliconFlow (ç¡…åŸºæµåŠ¨) ä½¿ç”¨ OpenAI å…¼å®¹ API
                if not api_key:
                    raise ValueError("SiliconFlowéœ€è¦API Key")
                client = create_openai_client(api_key=api_key, base_url="https://api.siliconflow.cn/v1")
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": prompt_content},
                        {"role": "user", "content": text},
                    ]
                )
                translated_text = response.choices[0].message.content.strip()
                
            elif model_provider == 'deepseek':
                # DeepSeek ä¹Ÿä½¿ç”¨ OpenAI å…¼å®¹ API
                if not api_key:
                    raise ValueError("DeepSeekéœ€è¦API Key")
                client = create_openai_client(api_key=api_key, base_url="https://api.deepseek.com/v1")
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": prompt_content},
                        {"role": "user", "content": text},
                    ]
                )
                translated_text = response.choices[0].message.content.strip()
                
            elif model_provider == 'volcano':
                # ç«å±±å¼•æ“ï¼Œä¹Ÿä½¿ç”¨ OpenAI å…¼å®¹ API
                if not api_key: raise ValueError("ç«å±±å¼•æ“éœ€è¦ API Key")
                client = create_openai_client(api_key=api_key, base_url="https://ark.cn-beijing.volces.com/api/v3")
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": prompt_content},
                        {"role": "user", "content": text},
                    ]
                )
                translated_text = response.choices[0].message.content.strip()

            elif model_provider == 'caiyun':
                if not api_key: raise ValueError("å½©äº‘å°è¯‘éœ€è¦ API Key")
                url = "http://api.interpreter.caiyunai.com/v1/translator"
                # ç¡®å®šç¿»è¯‘æ–¹å‘ï¼Œé»˜è®¤ä¸º auto2zhï¼ˆè‡ªåŠ¨æ£€æµ‹æºè¯­è¨€ç¿»è¯‘åˆ°ä¸­æ–‡ï¼‰
                trans_type = "auto2zh"
                if target_language == 'en':
                    trans_type = "zh2en"
                elif target_language == 'ja':
                    trans_type = "zh2ja"
                # ä¹Ÿå¯ä»¥åŸºäºæºè¯­è¨€ç¡®å®šç¿»è¯‘æ–¹å‘
                if 'japan' in model_name or 'ja' in model_name:
                    trans_type = "ja2zh"
                elif 'en' in model_name:
                    trans_type = "en2zh"
                
                headers = {
                    "Content-Type": "application/json",
                    "X-Authorization": f"token {api_key}"
                }
                payload = {
                    "source": [text],
                    "trans_type": trans_type,
                    "request_id": f"comic_translator_{int(time.time())}",
                    "detect": True,
                    "media": "text"
                }
                
                response = requests.post(url, headers=headers, json=payload)
                response.raise_for_status()
                result = response.json()
                if "target" in result and len(result["target"]) > 0:
                    translated_text = result["target"][0].strip()
                else:
                    raise ValueError(f"å½©äº‘å°è¯‘è¿”å›æ ¼å¼é”™è¯¯: {result}")

            elif model_provider == 'sakura':
                url = "http://localhost:8080/v1/chat/completions"
                headers = {"Content-Type": "application/json"}
                sakura_prompt = "ä½ æ˜¯ä¸€ä¸ªè½»å°è¯´ç¿»è¯‘æ¨¡å‹ï¼Œå¯ä»¥æµç•…é€šé¡ºåœ°ä»¥æ—¥æœ¬è½»å°è¯´çš„é£æ ¼å°†æ—¥æ–‡ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ï¼Œå¹¶è”ç³»ä¸Šä¸‹æ–‡æ­£ç¡®ä½¿ç”¨äººç§°ä»£è¯ï¼Œä¸æ“…è‡ªæ·»åŠ åŸæ–‡ä¸­æ²¡æœ‰çš„ä»£è¯ã€‚"
                payload = {
                    "model": model_name,
                    "messages": [
                        {"role": "system", "content": sakura_prompt},
                        {"role": "user", "content": f"å°†ä¸‹é¢çš„æ—¥æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼š{text}"}
                    ]
                }
                response = requests.post(url, headers=headers, json=payload)
                response.raise_for_status()
                result = response.json()
                choices = result.get('choices', [])
                if not choices:
                    raise ValueError("Sakura è¿”å›ç©º choices")
                translated_text = choices[0]['message']['content'].strip()

            elif model_provider == 'ollama':
                url = "http://localhost:11434/api/chat"
                payload = {
                    "model": model_name,
                    "messages": [
                        {"role": "system", "content": prompt_content},
                        {"role": "user", "content": text}
                    ],
                    "stream": False
                }
                response = requests.post(url, json=payload)
                response.raise_for_status()
                result = response.json()
                if "message" in result and "content" in result["message"]:
                    translated_text = result["message"]["content"].strip()
                else:
                    raise ValueError(f"Ollamaè¿”å›æ ¼å¼é”™è¯¯: {result}")
                    
            elif model_provider == constants.BAIDU_TRANSLATE_ENGINE_ID:
                # ç™¾åº¦ç¿»è¯‘API
                if not api_key or (isinstance(api_key, str) and not api_key.strip()):
                    raise ValueError("ç™¾åº¦ç¿»è¯‘APIéœ€è¦appid")
                if not model_name or (isinstance(model_name, str) and not model_name.strip()):
                    raise ValueError("ç™¾åº¦ç¿»è¯‘APIéœ€è¦appkey")
                    
                # è®¾ç½®ç™¾åº¦ç¿»è¯‘æ¥å£çš„è®¤è¯ä¿¡æ¯
                baidu_translate.set_credentials(api_key, model_name)
                
                # å°†é¡¹ç›®å†…éƒ¨è¯­è¨€ä»£ç è½¬æ¢ä¸ºç™¾åº¦ç¿»è¯‘APIæ”¯æŒçš„è¯­è¨€ä»£ç 
                from_lang = 'auto'  # é»˜è®¤è‡ªåŠ¨æ£€æµ‹æºè¯­è¨€
                to_lang = constants.PROJECT_TO_BAIDU_TRANSLATE_LANG_MAP.get(target_language, 'zh')
                
                # è°ƒç”¨ç™¾åº¦ç¿»è¯‘æ¥å£
                translated_text = baidu_translate.translate(text, from_lang, to_lang)
            
            elif model_provider == constants.YOUDAO_TRANSLATE_ENGINE_ID:
                # æœ‰é“ç¿»è¯‘API
                if not api_key or (isinstance(api_key, str) and not api_key.strip()):
                    raise ValueError("æœ‰é“ç¿»è¯‘APIéœ€è¦AppKey")
                if not model_name or (isinstance(model_name, str) and not model_name.strip()):
                    raise ValueError("æœ‰é“ç¿»è¯‘APIéœ€è¦AppSecret")
                    
                # è®¾ç½®æœ‰é“ç¿»è¯‘æ¥å£çš„è®¤è¯ä¿¡æ¯
                youdao_translate.app_key = api_key
                youdao_translate.app_secret = model_name
                
                # å°†é¡¹ç›®å†…éƒ¨è¯­è¨€ä»£ç è½¬æ¢ä¸ºæœ‰é“ç¿»è¯‘APIæ”¯æŒçš„è¯­è¨€ä»£ç 
                from_lang = 'auto'  # é»˜è®¤è‡ªåŠ¨æ£€æµ‹æºè¯­è¨€
                to_lang = constants.PROJECT_TO_YOUDAO_TRANSLATE_LANG_MAP.get(target_language, 'zh-CHS')
                
                # è°ƒç”¨æœ‰é“ç¿»è¯‘æ¥å£
                translated_text = youdao_translate.translate(text, from_lang, to_lang)
            elif model_provider.lower() == 'gemini':
                if not api_key:
                    raise ValueError("Gemini éœ€è¦ API Key")
                if not model_name:
                    raise ValueError("Gemini éœ€è¦æ¨¡å‹åç§° (ä¾‹å¦‚ gemini-1.5-flash-latest)")

                client = create_openai_client(
                    api_key=api_key,
                    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"  # æ ¹æ®æ•™ç¨‹
                )
                
                gemini_messages = []
                # System prompt å¯¹äº Gemini çš„ OpenAI å…¼å®¹å±‚æ˜¯å¦æœ‰æ•ˆéœ€è¦æµ‹è¯•
                # æ•™ç¨‹ä¸­çš„ chat completion ç¤ºä¾‹åŒ…å« system role
                if prompt_content:
                    gemini_messages.append({"role": "system", "content": prompt_content})
                # ç”¨æˆ·è¾“å…¥æ˜¯å®é™…çš„å¾…ç¿»è¯‘æ–‡æœ¬
                gemini_messages.append({"role": "user", "content": text}) 

                logger.debug(f"Gemini æ–‡æœ¬ç¿»è¯‘è¯·æ±‚ (æ¨¡å‹: {model_name}): {json.dumps(gemini_messages, ensure_ascii=False)}")

                response = client.chat.completions.create(
                    model=model_name,
                    messages=gemini_messages,
                )
                translated_text = response.choices[0].message.content.strip()
                logger.info(f"Gemini æ–‡æœ¬ç¿»è¯‘æˆåŠŸï¼Œæ¨¡å‹: {model_name}")
                logger.info(f"Gemini ç¿»è¯‘ç»“æœ (å‰100å­—ç¬¦): {translated_text[:100]}")
            elif model_provider == constants.CUSTOM_OPENAI_PROVIDER_ID:
                if not api_key:
                    raise ValueError("è‡ªå®šä¹‰ OpenAI å…¼å®¹æœåŠ¡éœ€è¦ API Key")
                if not model_name:
                    raise ValueError("è‡ªå®šä¹‰ OpenAI å…¼å®¹æœåŠ¡éœ€è¦æ¨¡å‹åç§°")
                if not custom_base_url: # æ£€æŸ¥ custom_base_url
                    raise ValueError("è‡ªå®šä¹‰ OpenAI å…¼å®¹æœåŠ¡éœ€è¦ Base URL")

                logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰ OpenAI å…¼å®¹æœåŠ¡: Base URL='{custom_base_url}', Model='{model_name}'")
                client = create_openai_client(api_key=api_key, base_url=custom_base_url)  # ä½¿ç”¨è¾…åŠ©å‡½æ•°è‡ªåŠ¨å¤„ç†ä»£ç†
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": prompt_content},
                        {"role": "user", "content": text},
                    ],
                )
                translated_text = response.choices[0].message.content.strip()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç¿»è¯‘æœåŠ¡æä¾›å•†: {model_provider}")
            
            # å¦‚æœä½¿ç”¨ JSON æ ¼å¼ï¼Œå°è¯•ä»å“åº”ä¸­æå– translated_text å­—æ®µ
            if use_json_format:
                try:
                    extracted_text = _safely_extract_from_json(translated_text, "translated_text")
                    logger.info(f"æˆåŠŸä»JSONå“åº”ä¸­æå–ç¿»è¯‘æ–‡æœ¬: '{extracted_text[:50]}...'")
                    translated_text = extracted_text
                except Exception as e:
                    logger.warning(f"æ— æ³•å°†ç¿»è¯‘ç»“æœè§£æä¸ºJSONï¼Œå°†å°è¯•æå–æ–‡æœ¬ã€‚åŸå§‹å“åº”: {translated_text[:100]}")
                    translated_text = _safely_extract_from_json(translated_text, "translated_text")
            
            break
            
        except Exception as e:
            retry_count += 1
            error_message = str(e)
            logger.error(f"ç¿»è¯‘å¤±è´¥ï¼ˆå°è¯• {retry_count}/{max_retries}ï¼ŒæœåŠ¡å•†: {model_provider}ï¼‰: {error_message}", exc_info=True)
            translated_text = "ã€ç¿»è¯‘å¤±è´¥ã€‘è¯·æ£€æŸ¥ç»ˆç«¯ä¸­çš„é”™è¯¯æ—¥å¿—"
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_detail = e.response.json()
                    logger.error(f"{model_provider} API é”™è¯¯è¯¦æƒ…: {error_detail}")
                except json.JSONDecodeError:
                    logger.error(f"{model_provider} API åŸå§‹é”™è¯¯å“åº” (çŠ¶æ€ç  {e.response.status_code}): {e.response.text}")

            if "API key" in error_message or "appid" in error_message or "appkey" in error_message or "authentication" in error_message.lower() or "Base URL" in error_message: # æ–°å¢ "Base URL" æ£€æŸ¥
                break # å‡­è¯æˆ–é…ç½®é”™è¯¯ï¼Œä¸é‡è¯•
            if retry_count < max_retries:
                time.sleep(1)
    
    # è®°å½•ç¿»è¯‘ç»“æœ
    if translated_text == "ã€ç¿»è¯‘å¤±è´¥ã€‘è¯·æ£€æŸ¥ç»ˆç«¯ä¸­çš„é”™è¯¯æ—¥å¿—":
        logger.warning(f"æœ€ç»ˆç¿»è¯‘å¤±è´¥: '{text}' -> '{translated_text}'")
    else:
        logger.info(f"æœ€ç»ˆç¿»è¯‘æˆåŠŸ: '{text[:30]}...' -> '{translated_text[:30]}...'")
        
    return translated_text


# æ·»åŠ æµ‹è¯•ç”¨çš„ Mock ç¿»è¯‘æä¾›å•†
def translate_with_mock(text, target_language, api_key=None, model_name=None, prompt_content=None):
    """åªç”¨äºæµ‹è¯•çš„æ¨¡æ‹Ÿç¿»è¯‘æä¾›å•†"""
    if not text or not text.strip():
        return ""
        
    # ç®€å•æ·»åŠ ç›®æ ‡è¯­è¨€ä½œä¸ºå‰ç¼€
    translated = f"[æµ‹è¯•{target_language}] {text[:15]}..."
    
    # å¦‚æœæ–‡æœ¬ä¸ºæ—¥è¯­ï¼Œæ¨¡æ‹Ÿä¸€äº›ç®€å•çš„ç¿»è¯‘è§„åˆ™
    if text and any(ord(c) > 0x3000 for c in text):
        if target_language.lower() in ["chinese", "zh"]:
            translated = f"ä¸­æ–‡ç¿»è¯‘: {text[:15]}..."
        elif target_language.lower() in ["english", "en"]:
            translated = f"English translation: {text[:15]}..."
    
    logger.info(f"Mock ç¿»è¯‘: '{text[:20]}...' -> '{translated}'")
    return translated


def _assemble_batch_prompt(texts: list, custom_prompt: str = None, use_json_format: bool = False) -> tuple:
    """
    å°†å¤šä¸ªæ–‡æœ¬ç»„è£…æˆæ‰¹é‡ç¿»è¯‘çš„ prompt
    
    Args:
        texts: å¾…ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
        custom_prompt: è‡ªå®šä¹‰æç¤ºè¯ (å¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤æ‰¹é‡ç¿»è¯‘æ¨¡æ¿)
        use_json_format: æ˜¯å¦ä½¿ç”¨ JSON è¾“å‡ºæ ¼å¼
        
    Returns:
        tuple: (messages_list, batch_size) - æ¶ˆæ¯åˆ—è¡¨å’Œæ‰¹æ¬¡å¤§å°
    """
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = []
    
    if use_json_format:
        # --- JSON æ¨¡å¼ ---
        # 1. System prompt
        if custom_prompt:
            system_prompt = custom_prompt
        else:
            system_prompt = constants.BATCH_TRANSLATE_JSON_SYSTEM_TEMPLATE
        messages.append({"role": "system", "content": system_prompt})
        
        # 2. Few-shot learning: JSON æ ¼å¼ç¤ºä¾‹
        if hasattr(constants, 'BATCH_TRANSLATE_JSON_SAMPLE_INPUT') and hasattr(constants, 'BATCH_TRANSLATE_JSON_SAMPLE_OUTPUT'):
            messages.append({"role": "user", "content": constants.BATCH_TRANSLATE_JSON_SAMPLE_INPUT})
            messages.append({"role": "assistant", "content": constants.BATCH_TRANSLATE_JSON_SAMPLE_OUTPUT})
            logger.debug("å·²æ·»åŠ  JSON æ¨¡å¼ç¿»è¯‘ç¤ºä¾‹")
        
        # 3. User promptï¼šæ„å»º JSON æ ¼å¼çš„è¾“å…¥
        import json
        texts_json = {"texts": [{"id": i+1, "text": text} for i, text in enumerate(texts)]}
        user_prompt = constants.BATCH_TRANSLATE_JSON_USER_TEMPLATE + "\n" + json.dumps(texts_json, ensure_ascii=False, indent=2)
        messages.append({"role": "user", "content": user_prompt})
    else:
        # --- çº¯æ–‡æœ¬æ¨¡å¼ (é»˜è®¤) ---
        # 1. System prompt
        if custom_prompt:
            system_prompt = custom_prompt
        else:
            system_prompt = constants.BATCH_TRANSLATE_SYSTEM_TEMPLATE
        messages.append({"role": "system", "content": system_prompt})
        
        # 2. Few-shot learning: æ·»åŠ ç¿»è¯‘ç¤ºä¾‹
        if hasattr(constants, 'BATCH_TRANSLATE_SAMPLE_INPUT') and hasattr(constants, 'BATCH_TRANSLATE_SAMPLE_OUTPUT'):
            messages.append({"role": "user", "content": constants.BATCH_TRANSLATE_SAMPLE_INPUT})
            messages.append({"role": "assistant", "content": constants.BATCH_TRANSLATE_SAMPLE_OUTPUT})
            logger.debug("å·²æ·»åŠ ç¿»è¯‘ç¤ºä¾‹")
        
        # 3. User promptï¼šå°†æ‰€æœ‰æ–‡æœ¬ç¼–å·å¹¶åˆå¹¶
        user_prompt = constants.BATCH_TRANSLATE_USER_TEMPLATE
        for i, text in enumerate(texts):
            user_prompt += f"\n<|{i+1}|>{text}"
        messages.append({"role": "user", "content": user_prompt})
    
    return messages, len(texts)




def _parse_batch_response(response_text: str, expected_count: int) -> list:
    """
    è§£ææ‰¹é‡ç¿»è¯‘çš„å“åº”
    
    Args:
        response_text: LLM è¿”å›çš„å“åº”æ–‡æœ¬
        expected_count: æœŸæœ›çš„ç¿»è¯‘æ•°é‡
        
    Returns:
        list: è§£æåçš„ç¿»è¯‘åˆ—è¡¨
        
    Raises:
        TranslationParseException: å½“æ— æ³•è§£æå‡ºæœ‰æ•ˆå†…å®¹æ—¶æŠ›å‡ºï¼Œè§¦å‘é‡è¯•
    """
    # --- å“åº”æ¸…ç† ---
    
    # 1. å»é™¤ <think>...</think> æ ‡ç­¾åŠå†…å®¹ (æŸäº›æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹)
    cleaned_text = re.sub(r'(</think>)?<think>.*?</think>', '', response_text, flags=re.DOTALL)
    
    # 2. åˆ é™¤å¤šä½™çš„ç©ºè¡Œ
    cleaned_text = re.sub(r'\n\s*\n', '\n', cleaned_text).strip()
    
    # 3. ä»…ä¿ç•™ <|1|> åˆ° <|max|> èŒƒå›´å†…çš„è¡Œï¼Œåˆ é™¤å‰åçš„è§£é‡Šæ€§æ–‡å­—
    lines = cleaned_text.splitlines()
    min_index_line = -1
    max_index_line = -1
    has_numeric_prefix = False
    
    for index, line in enumerate(lines):
        match = re.search(r'<\|(\d+)\|>', line)
        if match:
            has_numeric_prefix = True
            current_index = int(match.group(1))
            if current_index == 1:
                min_index_line = index
            if max_index_line == -1:
                max_index_line = index
            else:
                prev_match = re.search(r'<\|(\d+)\|>', lines[max_index_line])
                if prev_match and current_index > int(prev_match.group(1)):
                    max_index_line = index
    
    # ğŸ” æ–°å¢ï¼šæ£€æµ‹æ˜¯å¦å®Œå…¨æ— æ³•æ‰¾åˆ°ç¼–å·æ ¼å¼
    if not has_numeric_prefix:
        logger.warning(f"å“åº”ä¸­æœªæ‰¾åˆ° <|n|> æ ¼å¼çš„ç¼–å·ï¼Œæ— æ³•è§£æã€‚å“åº”å†…å®¹: {response_text[:200]}...")
        raise TranslationParseException(
            f"æ— æ³•åœ¨å“åº”ä¸­æ‰¾åˆ°æ‰¹é‡ç¿»è¯‘çš„ç¼–å·æ ¼å¼ <|n|>ï¼ŒAI å¯èƒ½æœªæŒ‰è¦æ±‚è¾“å‡º"
        )
    
    if has_numeric_prefix and min_index_line != -1:
        # åªä¿ç•™ä» <|1|> å¼€å§‹åˆ°æœ€å¤§ç¼–å·è¡Œçš„å†…å®¹
        modified_lines = lines[min_index_line:max_index_line + 1]
        cleaned_text = "\n".join(modified_lines)
    
    # 4. ä¿®å¤å‰ç¼€å’Œç¿»è¯‘å†…å®¹ä¹‹é—´çš„ç©ºæ ¼é—®é¢˜
    fixed_lines = []
    for line in cleaned_text.strip().split('\n'):
        # åŒ¹é… <|æ•°å­—|> å‰ç¼€æ ¼å¼ï¼Œå»é™¤å‰ç¼€åçš„å¤šä½™ç©ºæ ¼
        match = re.match(r'^(<\|\d+\|>)\s+(.*)$', line.strip())
        if match:
            prefix = match.group(1)
            content = match.group(2)
            fixed_lines.append(f"{prefix}{content}")
        else:
            fixed_lines.append(line)
    cleaned_text = '\n'.join(fixed_lines)
    
    # --- åˆ†å‰²è§£æ ---
    
    # ç‰¹æ®Šæƒ…å†µï¼šå•ä¸ªæŸ¥è¯¢ä½†å“åº”å¯èƒ½è¢«åˆ†æˆå¤šæ®µ (åœ¨åˆ†å‰²å‰æ£€æŸ¥)
    if expected_count == 1:
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤šä¸ªç¼–å·
        all_indices = re.findall(r'<\|(\d+)\|>', cleaned_text)
        if len(all_indices) > 1:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶…è¿‡ 1 çš„ç´¢å¼•ï¼ˆè¯´æ˜æ¨¡å‹é”™è¯¯åœ°åˆ†å‰²äº†å•ä¸ªç¿»è¯‘ï¼‰
            has_invalid = any(int(idx) > 1 for idx in all_indices)
            if has_invalid:
                # åˆå¹¶æ‰€æœ‰ç¿»è¯‘ï¼Œç§»é™¤æ‰€æœ‰ç¼–å·
                merged = re.sub(r'<\|\d+\|>', '', cleaned_text).strip()
                logger.warning("æ£€æµ‹åˆ°å•æŸ¥è¯¢è¢«åˆ†å‰²ï¼Œå·²åˆå¹¶ç¿»è¯‘ç»“æœ")
                return [merged]
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å“åº”ï¼š<|1|>...<|2|>...
    translations = re.split(r'<\|\d+\|>', cleaned_text)
    
    # æ¸…ç†æ¯ä¸ªç¿»è¯‘çš„å‰åç©ºæ ¼
    translations = [t.strip() for t in translations]
    
    # ç§»é™¤ç¬¬ä¸€ä¸ªç©ºå…ƒç´ ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if translations and not translations[0]:
        translations = translations[1:]
    
    # ğŸ” æ–°å¢ï¼šéªŒè¯è§£æç»“æœ
    if not translations:
        logger.warning("è§£æåæœªè·å–åˆ°ä»»ä½•ç¿»è¯‘å†…å®¹")
        raise TranslationParseException("è§£æåçš„ç¿»è¯‘åˆ—è¡¨ä¸ºç©ºï¼ŒAI å¯èƒ½è¿”å›äº†æ— æ•ˆå†…å®¹")
    
    return translations



def _parse_batch_json_response(response_text: str, expected_count: int) -> list:
    """
    è§£æ JSON æ ¼å¼çš„æ‰¹é‡ç¿»è¯‘å“åº”
    
    Args:
        response_text: LLM è¿”å›çš„å“åº”æ–‡æœ¬ (åº”ä¸º JSON æ ¼å¼)
        expected_count: æœŸæœ›çš„ç¿»è¯‘æ•°é‡
        
    Returns:
        list: è§£æåçš„ç¿»è¯‘åˆ—è¡¨
        
    Raises:
        TranslationParseException: å½“ JSON è§£æå¤±è´¥æ—¶æŠ›å‡ºï¼Œè§¦å‘é‡è¯•
    """
    import json
    
    # 1. å»é™¤ <think>...</think> æ ‡ç­¾åŠå†…å®¹
    cleaned_text = re.sub(r'(</think>)?<think>.*?</think>', '', response_text, flags=re.DOTALL)
    
    # 2. å°è¯•æå– JSON éƒ¨åˆ†ï¼ˆå¯èƒ½è¢«åŒ…è£¹åœ¨ ```json ... ``` ä¸­ï¼‰
    json_match = re.search(r'```json\s*([\s\S]*?)\s*```', cleaned_text)
    if json_match:
        json_str = json_match.group(1)
    else:
        # å°è¯•ç›´æ¥æ‰¾åˆ° JSON å¯¹è±¡
        json_match = re.search(r'\{[\s\S]*\}', cleaned_text)
        if json_match:
            json_str = json_match.group(0)
        else:
            logger.warning("æ— æ³•ä»å“åº”ä¸­æå– JSON")
            # ğŸ” ä¿®æ”¹ï¼šä¸å†é™çº§ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
            raise TranslationParseException("å“åº”ä¸­æœªæ‰¾åˆ° JSON æ ¼å¼çš„å†…å®¹")
    
    # 3. è§£æ JSON
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError as e:
        logger.warning(f"JSON è§£æå¤±è´¥: {e}")
        # ğŸ” ä¿®æ”¹ï¼šä¸å†é™çº§ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
        raise TranslationParseException(f"JSON è§£æå¤±è´¥: {e}")
    
    # 4. æå–ç¿»è¯‘ç»“æœ
    translations = []
    
    # æ”¯æŒä¸¤ç§æ ¼å¼:
    # æ ¼å¼1: {"translations": [{"id": 1, "text": "..."}, ...]}
    # æ ¼å¼2: {"TextList": [{"ID": 1, "text": "..."}, ...]} (å¤‡ç”¨æ ¼å¼)
    
    if 'translations' in data:
        items = data['translations']
    elif 'TextList' in data:
        items = data['TextList']
    else:
        logger.warning("JSON æ ¼å¼ä¸æ­£ç¡®ï¼Œæ‰¾ä¸åˆ° translations æˆ– TextList å­—æ®µ")
        # ğŸ” ä¿®æ”¹ï¼šä¸å†é™çº§ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
        raise TranslationParseException(
            f"JSON æ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›åŒ…å« 'translations' æˆ– 'TextList' å­—æ®µï¼Œå®é™…æ”¶åˆ°: {list(data.keys())}"
        )
    
    # æŒ‰ id æ’åºå¹¶æå–æ–‡æœ¬
    try:
        # ç»Ÿä¸€ id å­—æ®µåç§° (æ”¯æŒ 'id' å’Œ 'ID')
        for item in items:
            item_id = item.get('id') or item.get('ID')
            item_text = item.get('text', '')
            translations.append((item_id, item_text))
        
        # æŒ‰ id æ’åº
        translations.sort(key=lambda x: x[0] if x[0] else 0)
        translations = [t[1] for t in translations]
        
    except Exception as e:
        logger.warning(f"æå–ç¿»è¯‘ç»“æœå¤±è´¥: {e}")
        # ğŸ” ä¿®æ”¹ï¼šä¸å†é™çº§ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
        raise TranslationParseException(f"ä» JSON æå–ç¿»è¯‘ç»“æœå¤±è´¥: {e}")
    
    logger.debug(f"JSON æ¨¡å¼è§£ææˆåŠŸ: {len(translations)} æ¡ç¿»è¯‘")
    return translations


def _translate_batch_with_llm(texts: list, model_provider: str,
                               api_key: str, model_name: str, custom_prompt: str = None,
                               custom_base_url: str = None, max_retries: int = 2,
                               use_json_format: bool = False) -> list:
    """
    ä½¿ç”¨ LLM è¿›è¡Œæ‰¹é‡ç¿»è¯‘
    
    Args:
        texts: å¾…ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
        model_provider: æ¨¡å‹æä¾›å•†
        api_key: API å¯†é’¥
        model_name: æ¨¡å‹åç§°
        custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
        custom_base_url: è‡ªå®šä¹‰ API Base URL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        use_json_format: æ˜¯å¦ä½¿ç”¨ JSON è¾“å‡ºæ ¼å¼
        
    Returns:
        list: ç¿»è¯‘ç»“æœåˆ—è¡¨
    """
    if not texts:
        return []
    
    # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    results = [''] * len(texts)
    
    # ç»„è£…æ¶ˆæ¯åˆ—è¡¨ (åŒ…å« system promptã€few-shot ç¤ºä¾‹ã€user prompt)
    messages, batch_size = _assemble_batch_prompt(texts, custom_prompt, use_json_format)
    
    logger.info(f"æ‰¹é‡ç¿»è¯‘è¯·æ±‚: {batch_size} ä¸ªæ–‡æœ¬ç‰‡æ®µ (æ¶ˆæ¯æ•°: {len(messages)})")
    
    # ç¡®å®š API å®¢æˆ·ç«¯é…ç½®
    base_url_map = {
        'siliconflow': 'https://api.siliconflow.cn/v1',
        'deepseek': 'https://api.deepseek.com/v1',
        'volcano': 'https://ark.cn-beijing.volces.com/api/v3',
        constants.CUSTOM_OPENAI_PROVIDER_ID: custom_base_url,
    }
    
    # Gemini ä½¿ç”¨ç‰¹æ®Šçš„ base_url
    if model_provider.lower() == 'gemini':
        base_url = 'https://generativelanguage.googleapis.com/v1beta/openai/'
    else:
        base_url = base_url_map.get(model_provider, custom_base_url)
    
    if not base_url and model_provider not in ['ollama', 'sakura']:
        logger.error(f"æœªçŸ¥çš„æ¨¡å‹æä¾›å•†: {model_provider}")
        return texts  # è¿”å›åŸæ–‡
    
    # é‡è¯•å¾ªç¯
    for attempt in range(max_retries + 1):
        response_text = None  # åˆå§‹åŒ–å“åº”æ–‡æœ¬
        
        try:
            # === API è°ƒç”¨é˜¶æ®µ ===
            if model_provider == 'ollama':
                # Ollama ç‰¹æ®Šå¤„ç†
                url = "http://localhost:11434/api/chat"
                payload = {
                    "model": model_name,
                    "messages": messages,
                    "stream": False
                }
                response = requests.post(url, json=payload, timeout=120)
                response.raise_for_status()
                result = response.json()
                response_text = result.get("message", {}).get("content", "").strip()
                
            elif model_provider == 'sakura':
                # Sakura ç‰¹æ®Šå¤„ç†
                url = "http://localhost:8080/v1/chat/completions"
                headers = {"Content-Type": "application/json"}
                payload = {
                    "model": model_name,
                    "messages": messages
                }
                response = requests.post(url, headers=headers, json=payload, timeout=120)
                response.raise_for_status()
                result = response.json()
                choices = result.get('choices', [])
                if not choices:
                    raise ValueError("Sakura è¿”å›ç©º choices")
                response_text = choices[0]['message']['content'].strip()
                
            else:
                # OpenAI å…¼å®¹ API
                client = create_openai_client(api_key=api_key, base_url=base_url)
                response = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    timeout=120
                )
                response_text = response.choices[0].message.content.strip()
            
            # æ—¥å¿—ï¼šè¾“å‡ºåŸå§‹å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            logger.info(f"æ‰¹é‡ç¿»è¯‘å“åº”ï¼ˆå‰300å­—ç¬¦ï¼‰:\n{response_text[:300]}...")
            
            # === è§£æé˜¶æ®µ ===
            # æ ¹æ®æ¨¡å¼é€‰æ‹©è§£æå‡½æ•°ï¼ˆå¯èƒ½æŠ›å‡º TranslationParseExceptionï¼‰
            if use_json_format:
                translations = _parse_batch_json_response(response_text, len(texts))
            else:
                translations = _parse_batch_response(response_text, len(texts))
            
            # æ—¥å¿—ï¼šè¾“å‡ºè§£æç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
            logger.info(f"è§£æåçš„ç¿»è¯‘ç»“æœ: {translations}")
            
            # === éªŒè¯é˜¶æ®µ ===
            # éªŒè¯å“åº”æ•°é‡
            if len(translations) != len(texts):
                logger.warning(f"[å°è¯• {attempt+1}/{max_retries+1}] ç¿»è¯‘æ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(texts)}, å®é™… {len(translations)}")
                
                # å¦‚æœç¿»è¯‘æ•°é‡å°‘äºæœŸæœ›ï¼Œå¡«å…… [ç¿»è¯‘å¤±è´¥] æ ‡è®°
                if len(translations) < len(texts):
                    translations.extend(['ã€ç¿»è¯‘å¤±è´¥ã€‘è¯·æ£€æŸ¥ç»ˆç«¯ä¸­çš„é”™è¯¯æ—¥å¿—'] * (len(texts) - len(translations)))
                # å¦‚æœç¿»è¯‘æ•°é‡å¤šäºæœŸæœ›ï¼Œæˆªæ–­
                elif len(translations) > len(texts):
                    translations = translations[:len(texts)]
            
            # éªŒè¯éç©ºç¿»è¯‘
            empty_count = sum(1 for src, trans in zip(texts, translations) 
                             if src.strip() and not trans.strip())
            if empty_count > 0:
                logger.warning(f"[å°è¯• {attempt+1}/{max_retries+1}] æ£€æµ‹åˆ° {empty_count} ä¸ªç©ºç¿»è¯‘")
                if attempt < max_retries:
                    time.sleep(1)
                    continue  # é‡è¯•
            
            # === æˆåŠŸé˜¶æ®µ ===
            # è¿”å›ç»“æœï¼ˆç©ºç¿»è¯‘ä½¿ç”¨ [ç¿»è¯‘å¤±è´¥] æ ‡è®°ï¼‰
            for i, trans in enumerate(translations):
                results[i] = trans if trans else 'ã€ç¿»è¯‘å¤±è´¥ã€‘è¯·æ£€æŸ¥ç»ˆç«¯ä¸­çš„é”™è¯¯æ—¥å¿—'
            
            logger.info(f"æ‰¹é‡ç¿»è¯‘æˆåŠŸ: {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
            return results
        
        # ğŸ” æ–°å¢ï¼šä¸“é—¨æ•è·è§£æå¼‚å¸¸ï¼ˆä¼˜å…ˆçº§é«˜ï¼Œå…ˆæ•è·ï¼‰
        except TranslationParseException as parse_error:
            logger.error(f"[å°è¯• {attempt+1}/{max_retries+1}] æ‰¹é‡ç¿»è¯‘è§£æå¤±è´¥: {parse_error}")
            if attempt < max_retries:
                logger.info(f"è§£æå¤±è´¥ï¼Œç­‰å¾… 1 ç§’åé‡è¯•...")
                time.sleep(1)
                continue  # é‡è¯•æ•´ä¸ªæµç¨‹
            else:
                # é‡è¯•ç”¨å®Œï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›åŸæ–‡
                logger.error(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè§£æé”™è¯¯: {parse_error}")
                break
        
        # API è°ƒç”¨æˆ–å…¶ä»–å¼‚å¸¸
        except Exception as e:
            logger.error(f"[å°è¯• {attempt+1}/{max_retries+1}] æ‰¹é‡ç¿»è¯‘å¤±è´¥: {e}", exc_info=True)
            if attempt < max_retries:
                time.sleep(1)
                continue  # é‡è¯•
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› [ç¿»è¯‘å¤±è´¥] æ ‡è®°
    logger.error("æ‰¹é‡ç¿»è¯‘æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å› [ç¿»è¯‘å¤±è´¥] æ ‡è®°")
    return ['ã€ç¿»è¯‘å¤±è´¥ã€‘è¯·æ£€æŸ¥ç»ˆç«¯ä¸­çš„é”™è¯¯æ—¥å¿—'] * len(texts)


def translate_text_list(texts, target_language, model_provider, 
                        api_key=None, model_name=None, prompt_content=None, 
                        use_json_format=False, custom_base_url=None,
                        rpm_limit_translation: int = constants.DEFAULT_rpm_TRANSLATION,
                        max_retries: int = constants.DEFAULT_TRANSLATION_MAX_RETRIES):
    """
    ç¿»è¯‘æ–‡æœ¬åˆ—è¡¨ - ä½¿ç”¨æ‰¹é‡ç¿»è¯‘ç­–ç•¥
    
    å°†ä¸€é¡µå†…æ‰€æœ‰æ°”æ³¡çš„æ–‡æœ¬åˆå¹¶ä¸ºä¸€ä¸ªè¯·æ±‚å‘é€ç»™ LLMï¼Œä½¿ç”¨ <|n|> æ ¼å¼ç¼–å·ï¼Œ
    ä¸€æ¬¡ API è°ƒç”¨ç¿»è¯‘æ•´é¡µå†…å®¹ï¼Œå¤§å¹…æå‡æ•ˆç‡å’Œç¿»è¯‘ä¸€è‡´æ€§ã€‚
    
    æ³¨æ„ï¼šç›®æ ‡è¯­è¨€ç°åœ¨ç”±æç¤ºè¯æ§åˆ¶ï¼ˆé»˜è®¤ç¿»è¯‘ä¸ºä¸­æ–‡ï¼‰ï¼Œå¦‚éœ€ä¿®æ”¹è¯·ç¼–è¾‘ 
    constants.BATCH_TRANSLATE_SYSTEM_TEMPLATE ä¸­çš„æç¤ºè¯ã€‚

    Args:
        texts (list): åŒ…å«å¾…ç¿»è¯‘æ–‡æœ¬å­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚
        target_language (str): [å·²å¼ƒç”¨] ç›®æ ‡è¯­è¨€ä»£ç ï¼Œç°ç”±æç¤ºè¯æ§åˆ¶ã€‚
        model_provider (str): æ¨¡å‹æä¾›å•†ã€‚
        api_key (str, optional): API å¯†é’¥ã€‚
        model_name (str, optional): æ¨¡å‹åç§°ã€‚
        prompt_content (str, optional): è‡ªå®šä¹‰æç¤ºè¯ï¼Œå¯è¦†ç›–é»˜è®¤æç¤ºè¯ã€‚
        use_json_format (bool): æ˜¯å¦ä½¿ç”¨ JSON æ ¼å¼è¾“å‡ºã€‚True æ—¶ä½¿ç”¨ç»“æ„åŒ– JSON æ ¼å¼ï¼ŒFalse æ—¶ä½¿ç”¨ <|n|> ç¼–å·æ ¼å¼ã€‚
        custom_base_url (str, optional): ç”¨æˆ·è‡ªå®šä¹‰çš„ OpenAI å…¼å®¹ API çš„ Base URLã€‚
        rpm_limit_translation (int): ç¿»è¯‘æœåŠ¡çš„æ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶ã€‚
        max_retries (int): ç¿»è¯‘å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚
    Returns:
        list: åŒ…å«ç¿»è¯‘åæ–‡æœ¬çš„åˆ—è¡¨ï¼Œé¡ºåºä¸è¾“å…¥åˆ—è¡¨ä¸€è‡´ã€‚å¤±è´¥çš„é¡¹åŒ…å«é”™è¯¯ä¿¡æ¯ã€‚
    """
    if not texts:
        return []
    
    # è¿‡æ»¤ç©ºæ–‡æœ¬ï¼Œè®°å½•ç´¢å¼•
    non_empty_indices = []
    non_empty_texts = []
    final_translations = [''] * len(texts)
    
    for i, text in enumerate(texts):
        if text and text.strip():
            non_empty_indices.append(i)
            non_empty_texts.append(text)
        else:
            final_translations[i] = ''
    
    if not non_empty_texts:
        return final_translations
    
    logger.info(f"å¼€å§‹æ‰¹é‡ç¿»è¯‘ {len(non_empty_texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ (ä½¿ç”¨ {model_provider}, rpm: {rpm_limit_translation if rpm_limit_translation > 0 else 'æ— '})...")
    
    # ç‰¹æ®Šå¤„ç†æ¨¡æ‹Ÿç¿»è¯‘æä¾›å•†
    if model_provider.lower() == 'mock':
        logger.info("ä½¿ç”¨æ¨¡æ‹Ÿç¿»è¯‘æä¾›å•†")
        for i, text in enumerate(non_empty_texts):
            translated = translate_with_mock(
                text,
                target_language,
                api_key=api_key,
                model_name=model_name,
                prompt_content=prompt_content
            )
            final_translations[non_empty_indices[i]] = translated
        logger.info("æ‰¹é‡ç¿»è¯‘å®Œæˆã€‚")
        return final_translations
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒæ‰¹é‡ç¿»è¯‘çš„æä¾›å•† (LLM)
    llm_providers = {'siliconflow', 'deepseek', 'volcano', 'ollama', 'sakura', 'gemini', 
                     'custom_openai'}  # ä½¿ç”¨å­—ç¬¦ä¸²è€Œä¸æ˜¯å¸¸é‡ï¼Œä¾¿äº .lower() æ¯”è¾ƒ
    
    if model_provider.lower() in llm_providers:
        # --- rpm é™åˆ¶ ---
        _enforce_rpm_limit(
            rpm_limit_translation,
            f"BatchTranslation ({model_provider})",
            _translation_rpm_last_reset_time_container,
            _translation_rpm_request_count_container
        )
        
        # ä½¿ç”¨æ‰¹é‡ç¿»è¯‘
        # å°†æ–‡æœ¬æŒ‰å­—ç¬¦æ•°åˆ†æ‰¹ï¼Œé¿å…è¶…è¿‡ token é™åˆ¶
        max_chars = constants.BATCH_TRANSLATE_MAX_CHARS_PER_REQUEST
        batches = []
        current_batch = []
        current_chars = 0
        
        for text in non_empty_texts:
            text_len = len(text) + 10  # +10 ç”¨äº <|n|> æ ‡è®°
            if current_chars + text_len > max_chars and current_batch:
                batches.append(current_batch)
                current_batch = []
                current_chars = 0
            current_batch.append(text)
            current_chars += text_len
        
        if current_batch:
            batches.append(current_batch)
        
        logger.info(f"æ–‡æœ¬å·²åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œç¿»è¯‘")
        
        # ç¿»è¯‘æ¯ä¸ªæ‰¹æ¬¡
        all_translations = []
        for batch_idx, batch in enumerate(batches):
            logger.info(f"æ­£åœ¨ç¿»è¯‘æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)} ({len(batch)} ä¸ªæ–‡æœ¬)...")
            
            batch_translations = _translate_batch_with_llm(
                batch,
                model_provider,
                api_key,
                model_name,
                custom_prompt=prompt_content,
                custom_base_url=custom_base_url,
                max_retries=max_retries,
                use_json_format=use_json_format
            )
            all_translations.extend(batch_translations)
            
            # å¦‚æœæœ‰å¤šä¸ªæ‰¹æ¬¡ï¼Œåœ¨æ‰¹æ¬¡ä¹‹é—´ç¨å¾®ç­‰å¾…
            if len(batches) > 1 and batch_idx < len(batches) - 1:
                time.sleep(0.5)
        
        # å°†ç¿»è¯‘ç»“æœå†™å›æœ€ç»ˆåˆ—è¡¨
        for i, trans in enumerate(all_translations):
            if i < len(non_empty_indices):
                final_translations[non_empty_indices[i]] = trans
        
    else:
        # é LLM æä¾›å•† (å¦‚ç™¾åº¦ç¿»è¯‘ã€æœ‰é“ç¿»è¯‘)ï¼Œä½¿ç”¨åŸæœ‰çš„é€ä¸ªç¿»è¯‘é€»è¾‘
        logger.info(f"æä¾›å•† {model_provider} ä¸æ”¯æŒæ‰¹é‡ç¿»è¯‘ï¼Œä½¿ç”¨é€ä¸ªç¿»è¯‘æ¨¡å¼")
        for i, text in enumerate(non_empty_texts):
            translated = translate_single_text(
                text,
                target_language,
                model_provider,
                api_key=api_key,
                model_name=model_name,
                prompt_content=prompt_content,
                use_json_format=use_json_format,
                custom_base_url=custom_base_url,
                rpm_limit_translation=rpm_limit_translation,
                max_retries=max_retries
            )
            final_translations[non_empty_indices[i]] = translated
    
    logger.info(f"æ‰¹é‡ç¿»è¯‘å®Œæˆã€‚æˆåŠŸ {len([t for t in final_translations if t])} / {len(texts)}")
    return final_translations

# --- æµ‹è¯•ä»£ç  ---
if __name__ == '__main__':
    # è®¾ç½®åŸºæœ¬çš„æ—¥å¿—é…ç½®ï¼Œä»¥ä¾¿åœ¨æµ‹è¯•æ—¶æŸ¥çœ‹æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    print("--- æµ‹è¯•ç¿»è¯‘æ ¸å¿ƒé€»è¾‘ ---")
    test_text_jp = "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã€‚"
    test_text_en = "This is a test."

    # --- é…ç½®ä½ çš„æµ‹è¯• API Key å’Œæ¨¡å‹ ---
    test_api_key_sf = os.environ.get("TEST_SILICONFLOW_API_KEY", None)
    test_model_sf = "alibaba/Qwen1.5-14B-Chat"

    test_api_key_ds = os.environ.get("TEST_DEEPSEEK_API_KEY", None)
    test_model_ds = "deepseek-chat"

    test_api_key_volcano = os.environ.get("TEST_VOLCANO_API_KEY", None)
    test_model_volcano = "deepseek-v3-250324"

    test_model_ollama = "llama3"
    test_model_sakura = "sakura-14b-qwen2.5-v1.0"
    # ------------------------------------

    print(f"\næµ‹è¯• SiliconFlow ({test_model_sf}):")
    if test_api_key_sf:
        result_sf = translate_single_text(test_text_en, 'zh', 'siliconflow', test_api_key_sf, test_model_sf)
        print(f"  '{test_text_en}' -> '{result_sf}'")
    else:
        print("  è·³è¿‡ SiliconFlow æµ‹è¯•ï¼Œæœªè®¾ç½® TEST_SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ã€‚")

    print(f"\næµ‹è¯• DeepSeek ({test_model_ds}):")
    if test_api_key_ds:
        result_ds = translate_single_text(test_text_en, 'zh', 'deepseek', test_api_key_ds, test_model_ds)
        print(f"  '{test_text_en}' -> '{result_ds}'")
    else:
        print("  è·³è¿‡ DeepSeek æµ‹è¯•ï¼Œæœªè®¾ç½® TEST_DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ã€‚")
        
    # æµ‹è¯•ç™¾åº¦ç¿»è¯‘
    test_baidu_app_id = os.environ.get("TEST_BAIDU_TRANSLATE_APP_ID", None)
    test_baidu_app_key = os.environ.get("TEST_BAIDU_TRANSLATE_APP_KEY", None)
    
    print(f"\næµ‹è¯• ç™¾åº¦ç¿»è¯‘ API:")
    if test_baidu_app_id and test_baidu_app_key:
        result_baidu = translate_single_text(test_text_en, 'zh', constants.BAIDU_TRANSLATE_ENGINE_ID, test_baidu_app_id, test_baidu_app_key)
        print(f"  '{test_text_en}' -> '{result_baidu}'")
        
        result_baidu_jp = translate_single_text(test_text_jp, 'zh', constants.BAIDU_TRANSLATE_ENGINE_ID, test_baidu_app_id, test_baidu_app_key)
        print(f"  '{test_text_jp}' -> '{result_baidu_jp}'")
    else:
        print("  è·³è¿‡ç™¾åº¦ç¿»è¯‘æµ‹è¯•ï¼Œæœªè®¾ç½® TEST_BAIDU_TRANSLATE_APP_ID æˆ– TEST_BAIDU_TRANSLATE_APP_KEY ç¯å¢ƒå˜é‡ã€‚")

    print(f"\næµ‹è¯• ç«å±±å¼•æ“ ({test_model_volcano}):")
    if test_api_key_volcano:
        try:
            result_volcano = translate_single_text(test_text_en, 'zh', 'volcano', test_api_key_volcano, test_model_volcano)
            print(f"  '{test_text_en}' -> '{result_volcano}'")
        except Exception as e:
            print(f"  ç«å±±å¼•æ“æµ‹è¯•å‡ºé”™: {e}")
    else:
        print("  è·³è¿‡ç«å±±å¼•æ“æµ‹è¯•ï¼Œæœªè®¾ç½® TEST_VOLCANO_API_KEY ç¯å¢ƒå˜é‡ã€‚")

    print(f"\næµ‹è¯• Ollama ({test_model_ollama}):")
    try:
        requests.get("http://localhost:11434")
        result_ollama = translate_single_text(test_text_en, 'zh', 'ollama', model_name=test_model_ollama)
        print(f"  '{test_text_en}' -> '{result_ollama}'")
    except requests.exceptions.ConnectionError:
        print("  è·³è¿‡ Ollama æµ‹è¯•ï¼Œæ— æ³•è¿æ¥åˆ° http://localhost:11434ã€‚")
    except Exception as e:
         print(f"  Ollama æµ‹è¯•å‡ºé”™: {e}")

    print(f"\næµ‹è¯• Sakura ({test_model_sakura}):")
    try:
        requests.get("http://localhost:8080")
        result_sakura = translate_single_text(test_text_jp, 'zh', 'sakura', model_name=test_model_sakura)
        print(f"  '{test_text_jp}' -> '{result_sakura}'")
    except requests.exceptions.ConnectionError:
        print("  è·³è¿‡ Sakura æµ‹è¯•ï¼Œæ— æ³•è¿æ¥åˆ° http://localhost:8080ã€‚")
    except Exception as e:
         print(f"  Sakura æµ‹è¯•å‡ºé”™: {e}")

    print("\n--- æµ‹è¯•æ‰¹é‡ç¿»è¯‘ ---")
    test_list = ["Hello", "World", "ã“ã‚Œã¯ãƒšãƒ³ã§ã™"]
    # å°è¯•ä½¿ç”¨ Ollama è¿›è¡Œæ‰¹é‡æµ‹è¯•ï¼Œå¦‚æœ Ollama ä¸å¯ç”¨ï¼Œåˆ™æ­¤éƒ¨åˆ†ä¼šå¤±è´¥
    try:
        requests.get("http://localhost:11434")
        translated_list = translate_text_list(test_list, 'zh', 'ollama', model_name=test_model_ollama)
        print(f"æ‰¹é‡ç¿»è¯‘ç»“æœ ({len(translated_list)}):")
        for i, t in enumerate(translated_list):
            print(f"  '{test_list[i]}' -> '{t}'")
    except requests.exceptions.ConnectionError:
        print("  è·³è¿‡æ‰¹é‡ç¿»è¯‘æµ‹è¯•ï¼Œæ— æ³•è¿æ¥åˆ° Ollamaã€‚")
    except Exception as e:
        print(f"  æ‰¹é‡ç¿»è¯‘æµ‹è¯•å‡ºé”™: {e}")