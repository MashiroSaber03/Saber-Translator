"""
ç½‘é¡µæ¼«ç”»å¯¼å…¥ API

æä¾›ç½‘é¡µæ¼«ç”»å¯¼å…¥åŠŸèƒ½çš„ REST API æ¥å£ï¼š
- POST /api/web-import/extract - æå–æ¼«ç”»å›¾ç‰‡ (æ”¯æŒåŒå¼•æ“)
- POST /api/web-import/download - ä¸‹è½½å›¾ç‰‡ (æ”¯æŒåŒå¼•æ“)
- GET /api/web-import/check-support - æ£€æŸ¥ URL æ˜¯å¦æ”¯æŒ gallery-dl
- GET /api/web-import/proxy-image - ä»£ç†å›¾ç‰‡è¯·æ±‚ (è§£å†³é˜²ç›—é“¾)
- POST /api/web-import/test-firecrawl - æµ‹è¯• Firecrawl è¿æ¥
- POST /api/web-import/test-agent - æµ‹è¯• AI Agent è¿æ¥
"""

import logging
import json
import httpx
import threading
import time
from pathlib import Path
from datetime import datetime
from flask import Blueprint, request, jsonify, Response, stream_with_context, send_from_directory

from src.core.web_import import MangaScraperAgent, GalleryDLRunner, check_gallery_dl_support

logger = logging.getLogger("WebImportAPI")

web_import_bp = Blueprint('web_import', __name__, url_prefix='/api/web-import')

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent


@web_import_bp.route('/static/temp/gallery_dl/<path:filename>', methods=['GET'])
def serve_gallery_dl_temp(filename):
    """
    æä¾› gallery-dl ä¸´æ—¶æ–‡ä»¶çš„é™æ€è®¿é—®
    """
    temp_dir = PROJECT_ROOT / "data" / "temp" / "gallery_dl"
    return send_from_directory(str(temp_dir), filename)


@web_import_bp.route('/static/temp/ai_agent/<path:filename>', methods=['GET'])
def serve_ai_agent_temp(filename):
    """
    æä¾› AI Agent ä¸´æ—¶æ–‡ä»¶çš„é™æ€è®¿é—®
    """
    temp_dir = PROJECT_ROOT / "data" / "temp" / "ai_agent"
    return send_from_directory(str(temp_dir), filename)


@web_import_bp.route('/gallery-dl-images', methods=['GET'])
def get_gallery_dl_images():
    """
    è·å– gallery-dl ä¸´æ—¶ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡
    è¿”å› base64 ç¼–ç çš„å›¾ç‰‡æ•°æ®ï¼Œä¾›å‰ç«¯ç›´æ¥å¯¼å…¥
    """
    import base64
    
    temp_dir = PROJECT_ROOT / "data" / "temp" / "gallery_dl"
    
    if not temp_dir.exists():
        return jsonify({'success': False, 'error': 'ä¸´æ—¶ç›®å½•ä¸å­˜åœ¨', 'images': []})
    
    images = []
    all_files = list(temp_dir.glob('*'))
    all_files.sort(key=lambda p: p.name)
    
    for file_path in all_files:
        if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp']:
            try:
                with open(file_path, 'rb') as f:
                    img_data = f.read()
                ext = file_path.suffix.lower().lstrip('.')
                if ext == 'jpg':
                    ext = 'jpeg'
                base64_data = base64.b64encode(img_data).decode('utf-8')
                images.append({
                    'filename': file_path.name,
                    'data': f'data:image/{ext};base64,{base64_data}'
                })
            except Exception as e:
                logger.warning(f"è¯»å–å›¾ç‰‡å¤±è´¥ {file_path}: {e}")
    
    return jsonify({'success': True, 'images': images, 'total': len(images)})


@web_import_bp.route('/check-support', methods=['GET'])
def check_support():
    """
    æ£€æŸ¥ URL æ˜¯å¦æ”¯æŒ gallery-dl
    
    Query Parameters:
        url: è¦æ£€æŸ¥çš„ URL
    
    Response:
        {
            "available": true,   // gallery-dl æ˜¯å¦å¯ç”¨
            "supported": true    // URL æ˜¯å¦æ”¯æŒ
        }
    """
    url = request.args.get('url', '').strip()
    
    if not url:
        return jsonify({'available': False, 'supported': False})
    
    try:
        result = check_gallery_dl_support(url)
        return jsonify(result)
    except Exception as e:
        logger.exception("æ£€æŸ¥ gallery-dl æ”¯æŒå¤±è´¥")
        return jsonify({'available': False, 'supported': False, 'error': str(e)})


@web_import_bp.route('/proxy-image', methods=['GET'])
def proxy_image():
    """
    ä»£ç†å›¾ç‰‡è¯·æ±‚ï¼Œè§£å†³é˜²ç›—é“¾ 403 é—®é¢˜
    
    Query Parameters:
        url: å›¾ç‰‡ URL
        referer: Referer å¤´ (å¯é€‰)
    
    Response:
        å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®
    """
    url = request.args.get('url', '').strip()
    referer = request.args.get('referer', '').strip()
    
    if not url:
        return "No URL provided", 400
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
    }
    
    if referer:
        headers["Referer"] = referer
    
    try:
        with httpx.Client(timeout=30, follow_redirects=True) as client:
            resp = client.get(url, headers=headers)
            resp.raise_for_status()
            
            content_type = resp.headers.get('content-type', 'image/jpeg')
            return Response(resp.content, mimetype=content_type)
    except httpx.HTTPStatusError as e:
        logger.warning(f"ä»£ç†å›¾ç‰‡å¤±è´¥: {e.response.status_code} - {url}")
        return f"HTTP Error: {e.response.status_code}", e.response.status_code
    except Exception as e:
        logger.exception(f"ä»£ç†å›¾ç‰‡å¼‚å¸¸: {url}")
        return f"Proxy Error: {str(e)}", 502


@web_import_bp.route('/extract', methods=['POST'])
def extract_images():
    """
    æå–æ¼«ç”»å›¾ç‰‡ (æ”¯æŒåŒå¼•æ“)
    
    Request Body:
        {
            "url": "æ¼«ç”»ç½‘é¡µURL",
            "engine": "auto" | "gallery-dl" | "ai-agent",  // å¼•æ“é€‰æ‹©
            "config": { ... WebImportSettings }
        }
    
    Response (SSE Stream):
        event: log
        data: {"timestamp": "...", "type": "info", "message": "..."}
        
        event: result
        data: {"success": true, "comicTitle": "...", "engine": "gallery-dl", ...}
    """
    try:
        data = request.get_json()
        url = data.get('url', '').strip()
        engine = data.get('engine', 'auto')  # auto | gallery-dl | ai-agent
        config = data.get('config', {})
        
        if not url:
            return jsonify({'success': False, 'error': 'è¯·è¾“å…¥ç½‘å€'}), 400
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªå¼•æ“
        use_gallery_dl = False
        
        if engine == 'gallery-dl':
            use_gallery_dl = True
        elif engine == 'ai-agent':
            use_gallery_dl = False
        else:  # auto
            # è‡ªåŠ¨æ£€æµ‹ï¼šä¼˜å…ˆä½¿ç”¨ gallery-dl
            support_info = check_gallery_dl_support(url)
            use_gallery_dl = support_info.get('available') and support_info.get('supported')
        
        if use_gallery_dl:
            # ä½¿ç”¨ Gallery-DL å¼•æ“
            return _extract_with_gallery_dl(url, config)
        else:
            # ä½¿ç”¨ AI Agent å¼•æ“
            return _extract_with_ai_agent(url, config)
        
    except Exception as e:
        logger.exception("æå– API é”™è¯¯")
        return jsonify({'success': False, 'error': str(e)}), 500



def _extract_with_gallery_dl(url: str, config: dict):
    """ä½¿ç”¨ Gallery-DL å¼•æ“æå–"""
    
    def generate():
        try:
            # å‘é€å¼€å§‹æ—¥å¿—
            log_data = {
                'timestamp': datetime.now().strftime('%H:%M:%S'),
                'type': 'info',
                'message': f'ä½¿ç”¨ Gallery-DL å¼•æ“æå–: {url}'
            }
            yield f"event: log\ndata: {json.dumps(log_data, ensure_ascii=False)}\n\n"
            
            # åˆ›å»ºè¿è¡Œå™¨
            runner_config = {
                **config,
                'timeout': 600  # å›ºå®š10åˆ†é’Ÿè¶…æ—¶
            }
            runner = GalleryDLRunner(runner_config)
            
            # å‘é€æå–ä¸­æ—¥å¿—
            log_data = {
                'timestamp': datetime.now().strftime('%H:%M:%S'),
                'type': 'tool_call',
                'message': 'è°ƒç”¨ gallery-dl å¼€å§‹ä¸‹è½½å›¾ç‰‡...'
            }
            yield f"event: log\ndata: {json.dumps(log_data, ensure_ascii=False)}\n\n"
            
            # å‘é€ç­‰å¾…æç¤º
            log_data = {
                'timestamp': datetime.now().strftime('%H:%M:%S'),
                'type': 'info',
                'message': 'â³ å¼€å§‹ä¸‹è½½ï¼Œå›¾ç‰‡å°†å®æ—¶æ˜¾ç¤º...'
            }
            yield f"event: log\ndata: {json.dumps(log_data, ensure_ascii=False)}\n\n"
            
            # å®šä¹‰è¿›åº¦å›è°ƒï¼šæ¯ä¸‹è½½ä¸€å¼ å›¾ç‰‡å°±æ¨é€
            pages_yielded = []  # ç”¨äºæ”¶é›†å·²æ¨é€çš„é¡µé¢
            
            def on_page_downloaded(page_data):
                """æ¯å‘ç°ä¸€å¼ æ–°å›¾ç‰‡çš„å›è°ƒ"""
                pages_yielded.append(page_data)
            
            # æ‰§è¡Œæå–ï¼ˆå¸¦è¿›åº¦å›è°ƒï¼‰
            logger.info(f"å¼€å§‹ gallery-dl æå–: {url}")
            
            # ç”±äºå›è°ƒæ— æ³•ç›´æ¥åœ¨ç”Ÿæˆå™¨ä¸­ yieldï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨è½®è¯¢æ–¹å¼
            result_container = {'result': None, 'error': None}
            
            def extraction_thread():
                try:
                    result_container['result'] = runner.extract_metadata(url, on_progress=on_page_downloaded)
                except Exception as e:
                    result_container['error'] = e
            
            # å¯åŠ¨æå–çº¿ç¨‹
            thread = threading.Thread(target=extraction_thread, daemon=True)
            thread.start()
            
            # è½®è¯¢æ£€æŸ¥æ–°é¡µé¢å¹¶æ¨é€
            last_count = 0
            while thread.is_alive() or last_count < len(pages_yielded):
                # æ¨é€æ–°å‘ç°çš„é¡µé¢
                while last_count < len(pages_yielded):
                    page_data = pages_yielded[last_count]
                    page_event = {
                        'timestamp': datetime.now().strftime('%H:%M:%S'),
                        'type': 'info',
                        'message': f'ğŸ“¥ ç¬¬ {page_data["pageNumber"]} å¼ å›¾ç‰‡å·²ä¸‹è½½'
                    }
                    yield f"event: log\ndata: {json.dumps(page_event, ensure_ascii=False)}\n\n"
                    
                    # æ¨é€å›¾ç‰‡æ•°æ®
                    yield f"event: page\ndata: {json.dumps(page_data, ensure_ascii=False)}\n\n"
                    
                    last_count += 1
                
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´å†æ£€æŸ¥
                time.sleep(0.1)
            
            # ç­‰å¾…çº¿ç¨‹å®Œæˆ
            thread.join(timeout=1)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if result_container['error']:
                raise result_container['error']
            
            result = result_container['result']
            logger.info(f"gallery-dl æå–å®Œæˆ: success={result.success}, pages={result.total_pages}")
            
            # å‘é€å®Œæˆæ—¥å¿—
            if result.success:
                log_data = {
                    'timestamp': datetime.now().strftime('%H:%M:%S'),
                    'type': 'tool_result',
                    'message': f'âœ… æå–å®Œæˆ: å…± {result.total_pages} å¼ å›¾ç‰‡'
                }
            else:
                log_data = {
                    'timestamp': datetime.now().strftime('%H:%M:%S'),
                    'type': 'error',
                    'message': f'æå–å¤±è´¥: {result.error}'
                }
            yield f"event: log\ndata: {json.dumps(log_data, ensure_ascii=False)}\n\n"
            
            # å‘é€æœ€ç»ˆç»“æœ
            result_data = {
                'success': result.success,
                'comicTitle': result.comic_title,
                'chapterTitle': result.chapter_title,
                'pages': result.pages,
                'totalPages': result.total_pages,
                'sourceUrl': result.source_url,
                'referer': result.referer,
                'engine': 'gallery-dl',
                'error': result.error
            }
            yield f"event: result\ndata: {json.dumps(result_data, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.exception("Gallery-DL æå–è¿‡ç¨‹å‘ç”Ÿé”™è¯¯")
            error_log = {
                'timestamp': datetime.now().strftime('%H:%M:%S'),
                'type': 'error',
                'message': f'å¼‚å¸¸: {str(e)}'
            }
            yield f"event: log\ndata: {json.dumps(error_log, ensure_ascii=False)}\n\n"
            
            error_data = {
                'success': False,
                'engine': 'gallery-dl',
                'error': str(e)
            }
            yield f"event: error\ndata: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no',
            'Connection': 'keep-alive'
        }
    )


def _extract_with_ai_agent(url: str, config: dict):
    """ä½¿ç”¨ AI Agent å¼•æ“æå–"""
    
    # æ£€æŸ¥å¿…è¦çš„é…ç½®
    firecrawl_key = config.get('firecrawl', {}).get('apiKey', '')
    agent_key = config.get('agent', {}).get('apiKey', '')
    
    if not firecrawl_key:
        return jsonify({'success': False, 'error': 'è¯·é…ç½® Firecrawl API Key'}), 400
    if not agent_key:
        return jsonify({'success': False, 'error': 'è¯·é…ç½® AI Agent API Key'}), 400
    
    # ä¸´æ—¶ç›®å½•ï¼ˆä¸ gallery-dl ä½¿ç”¨ç›¸åŒçš„ç»“æ„ï¼Œä¾¿äºç»Ÿä¸€å¤„ç†ï¼‰
    temp_dir = Path("data/temp/ai_agent")
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¯æ¬¡æå–å‰æ¸…ç†æ—§æ–‡ä»¶
    for old_file in temp_dir.glob("*"):
        try:
            old_file.unlink()
        except:
            pass
    
    def generate():
        try:
            # åˆ›å»º Agent
            agent = MangaScraperAgent(config)
            
            # æ”¶é›†æ—¥å¿—çš„åŒ…è£…å™¨
            collected_logs = []
            def collect_log(log):
                collected_logs.append(log)
            
            # ç›´æ¥è°ƒç”¨åŒæ­¥ç‰ˆæœ¬çš„ extract æ–¹æ³•
            result = agent.extract(url, collect_log)
            
            # å‘é€æ”¶é›†çš„æ—¥å¿—
            for log in collected_logs:
                log_data = {
                    'timestamp': log.timestamp,
                    'type': log.type,
                    'message': log.message
                }
                yield f"event: log\ndata: {json.dumps(log_data, ensure_ascii=False)}\n\n"
            
            # å¦‚æœæå–æˆåŠŸï¼Œä¸‹è½½å›¾ç‰‡åˆ°ä¸´æ—¶ç›®å½•
            if result.success and result.pages:
                yield f"event: log\ndata: {json.dumps({'timestamp': datetime.now().strftime('%H:%M:%S'), 'type': 'info', 'message': f'å¼€å§‹ä¸‹è½½ {len(result.pages)} å¼ å›¾ç‰‡...'}, ensure_ascii=False)}\n\n"
                
                # ä»åŸå§‹ URL æå– Referer
                from urllib.parse import urlparse
                parsed = urlparse(url)
                referer = f"{parsed.scheme}://{parsed.netloc}/"
                
                # ä¸‹è½½æ¯å¼ å›¾ç‰‡
                downloaded_pages = []
                for i, page in enumerate(result.pages):
                    page_num = page.get('pageNumber', i + 1)
                    image_url = page.get('imageUrl', '')
                    
                    if not image_url or image_url.startswith('blob:'):
                        continue
                    
                    try:
                        # ä¸‹è½½å›¾ç‰‡
                        headers = {
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                            'Referer': referer
                        }
                        
                        with httpx.Client(timeout=30.0, follow_redirects=True) as client:
                            resp = client.get(image_url, headers=headers)
                            resp.raise_for_status()
                            
                            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
                            content_type = resp.headers.get('content-type', '')
                            if 'webp' in content_type:
                                ext = '.webp'
                            elif 'png' in content_type:
                                ext = '.png'
                            elif 'gif' in content_type:
                                ext = '.gif'
                            else:
                                ext = '.jpg'
                            
                            # ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
                            filename = f"{page_num:04d}{ext}"
                            filepath = temp_dir / filename
                            filepath.write_bytes(resp.content)
                            
                            # æ›´æ–°é¡µé¢æ•°æ®ï¼Œä½¿ç”¨æœ¬åœ°é™æ€è·¯å¾„
                            local_url = f"/api/web-import/static/temp/ai_agent/{filename}"
                            downloaded_pages.append({
                                'pageNumber': page_num,
                                'imageUrl': local_url,
                                'originalUrl': image_url  # ä¿ç•™åŸå§‹ URL ä»¥å¤‡ç”¨
                            })
                            
                            # æ¨é€è¿›åº¦
                            log_msg = {
                                'timestamp': datetime.now().strftime('%H:%M:%S'),
                                'type': 'info',
                                'message': f'ğŸ“¥ ç¬¬ {page_num} å¼ å›¾ç‰‡å·²ä¸‹è½½ ({i + 1}/{len(result.pages)})'
                            }
                            yield f"event: log\ndata: {json.dumps(log_msg, ensure_ascii=False)}\n\n"
                            
                            # æ¨é€é¡µé¢æ•°æ®ï¼ˆä¸ gallery-dl ç›¸åŒæ ¼å¼ï¼‰
                            page_data = {
                                'pageNumber': page_num,
                                'imageUrl': local_url,
                                'originalUrl': image_url
                            }
                            yield f"event: page\ndata: {json.dumps(page_data, ensure_ascii=False)}\n\n"
                            
                    except Exception as e:
                        logger.warning(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {image_url}: {e}")
                        # ä¸‹è½½å¤±è´¥æ—¶ä½¿ç”¨åŸå§‹ URL
                        downloaded_pages.append({
                            'pageNumber': page_num,
                            'imageUrl': image_url
                        })
                
                # æ›´æ–°ç»“æœä¸­çš„ pages
                if downloaded_pages:
                    result.pages = downloaded_pages
                    result.total_pages = len(downloaded_pages)
                
                yield f"event: log\ndata: {json.dumps({'timestamp': datetime.now().strftime('%H:%M:%S'), 'type': 'tool_result', 'message': f'âœ… å›¾ç‰‡ä¸‹è½½å®Œæˆ: {len(downloaded_pages)} å¼ '}, ensure_ascii=False)}\n\n"
            
            # å‘é€æœ€ç»ˆç»“æœ
            result_data = {
                'success': result.success,
                'comicTitle': result.comic_title,
                'chapterTitle': result.chapter_title,
                'pages': result.pages,
                'totalPages': result.total_pages,
                'sourceUrl': result.source_url,
                'referer': '',
                'engine': 'ai-agent',
                'error': result.error
            }
            yield f"event: result\ndata: {json.dumps(result_data, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.exception("AI Agent æå–è¿‡ç¨‹å‘ç”Ÿé”™è¯¯")
            error_data = {
                'success': False,
                'engine': 'ai-agent',
                'error': str(e)
            }
            yield f"event: error\ndata: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no'
        }
    )


@web_import_bp.route('/download', methods=['POST'])
def download_images():
    """
    ä¸‹è½½å›¾ç‰‡ (æ”¯æŒåŒå¼•æ“)
    
    Request Body:
        {
            "pages": [{"pageNumber": 1, "imageUrl": "..."}, ...],
            "sourceUrl": "æ¥æºé¡µé¢URL",
            "engine": "gallery-dl" | "ai-agent",  // ä½¿ç”¨çš„å¼•æ“
            "config": { ... WebImportSettings }
        }
    
    Response:
        {
            "success": true,
            "images": [{"index": 0, "filename": "...", "dataUrl": "...", "size": 123}, ...],
            "failedCount": 0
        }
    """
    try:
        data = request.get_json()
        pages = data.get('pages', [])
        source_url = data.get('sourceUrl', '')
        engine = data.get('engine', 'ai-agent')
        config = data.get('config', {})
        
        if not pages:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰è¦ä¸‹è½½çš„å›¾ç‰‡'}), 400
        
        # æ ¹æ®å¼•æ“é€‰æ‹©ä¸‹è½½æ–¹å¼
        if engine == 'gallery-dl':
            # ä½¿ç”¨ Gallery-DL æ‰˜ç®¡ä¸‹è½½
            return _download_with_gallery_dl(source_url, pages, config)
        else:
            # AI Agent æ¨¡å¼ï¼šæ£€æŸ¥å›¾ç‰‡æ˜¯å¦å·²åœ¨ä¸´æ—¶ç›®å½•
            # å¦‚æœ imageUrl ä»¥ /api/web-import/static/temp/ai_agent/ å¼€å¤´ï¼Œè¯´æ˜å·²ä¸‹è½½
            return _download_with_ai_agent_cache(pages, source_url, config)
        
    except Exception as e:
        logger.exception("ä¸‹è½½ API é”™è¯¯")
        return jsonify({'success': False, 'error': str(e)}), 500


def _download_with_ai_agent_cache(pages: list, source_url: str, config: dict):
    """
    ä½¿ç”¨ AI Agent ç¼“å­˜ä¸‹è½½å›¾ç‰‡
    å¦‚æœå›¾ç‰‡å·²åœ¨ä¸´æ—¶ç›®å½•ï¼Œç›´æ¥è¯»å–ï¼›å¦åˆ™é‡æ–°ä¸‹è½½
    """
    import base64
    
    temp_dir = Path("data/temp/ai_agent")
    images = []
    failed_count = 0
    
    # è·å–å›¾ç‰‡é¢„å¤„ç†é…ç½®
    preprocess = config.get('imagePreprocess', {})
    
    for i, page in enumerate(pages):
        page_num = page.get('pageNumber', i + 1)
        image_url = page.get('imageUrl', '')
        original_url = page.get('originalUrl', '')  # åŸå§‹è¿œç¨‹ URL
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°ç¼“å­˜æ–‡ä»¶
            if image_url.startswith('/api/web-import/static/temp/ai_agent/'):
                # ä»æœ¬åœ°è¯»å–
                filename = image_url.split('/')[-1]
                filepath = temp_dir / filename
                
                if filepath.exists():
                    # è¯»å–æœ¬åœ°æ–‡ä»¶
                    image_data = filepath.read_bytes()
                    
                    # åº”ç”¨å›¾ç‰‡é¢„å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if preprocess.get('enabled'):
                        from src.core.web_import.image_processor import ImageProcessor
                        processor = ImageProcessor(preprocess)
                        image_data = processor.process(image_data)
                    
                    # è½¬æ¢ä¸º base64
                    ext = filepath.suffix.lower()
                    mime_type = {
                        '.jpg': 'image/jpeg',
                        '.jpeg': 'image/jpeg',
                        '.png': 'image/png',
                        '.webp': 'image/webp',
                        '.gif': 'image/gif'
                    }.get(ext, 'image/jpeg')
                    
                    data_url = f"data:{mime_type};base64,{base64.b64encode(image_data).decode()}"
                    
                    images.append({
                        'index': page_num - 1,
                        'filename': f"page_{page_num:04d}{ext}",
                        'dataUrl': data_url,
                        'size': len(image_data)
                    })
                    logger.debug(f"ä»ç¼“å­˜è¯»å–å›¾ç‰‡: {filename}")
                    continue
            
            # ä¸æ˜¯æœ¬åœ°ç¼“å­˜ï¼Œä½¿ç”¨ ImageDownloader ä¸‹è½½
            # è¿™ç§æƒ…å†µä¸€èˆ¬æ˜¯ä¸‹è½½å¤±è´¥å›é€€åˆ°åŸå§‹ URL
            download_url = original_url if original_url else image_url
            if download_url and not download_url.startswith('blob:'):
                from src.core.web_import.image_downloader import ImageDownloader
                downloader = ImageDownloader(config.get('download', {}))
                result = downloader.download_single_sync(download_url, source_url)
                
                if result.get('success'):
                    images.append({
                        'index': page_num - 1,
                        'filename': f"page_{page_num:04d}.jpg",
                        'dataUrl': result.get('dataUrl', ''),
                        'size': result.get('size', 0)
                    })
                    continue
            
            # ä¸‹è½½å¤±è´¥
            failed_count += 1
            logger.warning(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {image_url}")
            
        except Exception as e:
            failed_count += 1
            logger.warning(f"å¤„ç†å›¾ç‰‡å¤±è´¥ {image_url}: {e}")
    
    return jsonify({
        'success': True,
        'images': images,
        'failedCount': failed_count
    })


def _download_with_gallery_dl(source_url: str, pages: list, config: dict):
    """ä½¿ç”¨ Gallery-DL æ‰˜ç®¡ä¸‹è½½å›¾ç‰‡"""
    # è·å–é€‰ä¸­çš„é¡µç 
    selected_indices = [p.get('pageNumber', i + 1) for i, p in enumerate(pages)]
    
    # æ„å»ºé…ç½®
    runner_config = {
        'timeout': config.get('download', {}).get('timeout', 600),
        'imagePreprocess': config.get('imagePreprocess', {})
    }
    
    # åˆ›å»ºè¿è¡Œå™¨
    runner = GalleryDLRunner(runner_config)
    
    # æ‰§è¡Œä¸‹è½½
    results = runner.download(source_url, selected_indices)
    
    # è½¬æ¢ç»“æœ
    images = []
    failed_count = 0
    
    for result in results:
        if result.get('success', False):
            images.append({
                'index': result.get('index', 0),
                'filename': result.get('filename', ''),
                'dataUrl': result.get('dataUrl', ''),
                'size': result.get('size', 0)
            })
        else:
            failed_count += 1
            logger.warning(f"Gallery-DL ä¸‹è½½å¤±è´¥: {result.get('error', 'Unknown error')}")
    
    return jsonify({
        'success': True,
        'images': images,
        'failedCount': failed_count
    })


@web_import_bp.route('/test-firecrawl', methods=['POST'])
def test_firecrawl_connection():
    """
    æµ‹è¯• Firecrawl è¿æ¥
    
    Request Body:
        {
            "apiKey": "Firecrawl API Key"
        }
    
    Response:
        {
            "success": true,
            "message": "è¿æ¥æˆåŠŸ"
        }
    """
    try:
        data = request.get_json()
        api_key = data.get('apiKey', '').strip()
        
        if not api_key:
            return jsonify({'success': False, 'error': 'è¯·è¾“å…¥ API Key'}), 400
        
        import httpx
        
        # æµ‹è¯• API è¿æ¥
        response = httpx.get(
            'https://api.firecrawl.dev/v1/scrape',
            headers={'Authorization': f'Bearer {api_key}'},
            timeout=10
        )
        
        # 401 è¡¨ç¤º API Key æ— æ•ˆï¼Œä½†è¿æ¥æˆåŠŸ
        # å…¶ä»–çŠ¶æ€ç å¯èƒ½è¡¨ç¤ºæœåŠ¡å¯ç”¨
        if response.status_code in (401, 403):
            return jsonify({'success': False, 'error': 'API Key æ— æ•ˆ'}), 400
        
        return jsonify({'success': True, 'message': 'è¿æ¥æˆåŠŸ'})
        
    except httpx.TimeoutException:
        return jsonify({'success': False, 'error': 'è¿æ¥è¶…æ—¶'}), 500
    except Exception as e:
        logger.exception("æµ‹è¯• Firecrawl è¿æ¥å¤±è´¥")
        return jsonify({'success': False, 'error': str(e)}), 500


@web_import_bp.route('/test-agent', methods=['POST'])
def test_agent_connection():
    """
    æµ‹è¯• AI Agent è¿æ¥
    
    Request Body:
        {
            "provider": "æœåŠ¡å•†",
            "apiKey": "API Key",
            "customBaseUrl": "è‡ªå®šä¹‰åœ°å€",
            "modelName": "æ¨¡å‹åç§°"
        }
    
    Response:
        {
            "success": true,
            "message": "è¿æ¥æˆåŠŸ"
        }
    """
    try:
        data = request.get_json()
        provider = data.get('provider', 'openai')
        api_key = data.get('apiKey', '').strip()
        base_url = data.get('customBaseUrl', '').strip()
        model_name = data.get('modelName', 'gpt-4o-mini')
        
        # æ—¥å¿—ï¼šæ¥æ”¶åˆ°çš„å‚æ•°
        logger.info(f"[Agentæµ‹è¯•] æœåŠ¡å•†: {provider}")
        logger.info(f"[Agentæµ‹è¯•] æ¨¡å‹åç§°: {model_name}")
        logger.info(f"[Agentæµ‹è¯•] è‡ªå®šä¹‰Base URL: {base_url if base_url else '(æœªè®¾ç½®)'}")
        logger.info(f"[Agentæµ‹è¯•] API Keyå‰ç¼€: {api_key[:15]}..." if len(api_key) > 15 else f"[Agentæµ‹è¯•] API Key: {api_key}")
        
        if not api_key:
            return jsonify({'success': False, 'error': 'è¯·è¾“å…¥ API Key'}), 400
        
        from openai import OpenAI
        
        # è·å– base_url
        provider_urls = {
            'openai': None,
            'siliconflow': 'https://api.siliconflow.cn/v1',
            'deepseek': 'https://api.deepseek.com/v1',
            'volcano': 'https://ark.cn-beijing.volces.com/api/v3',
            'gemini': 'https://generativelanguage.googleapis.com/v1beta/openai/'
        }
        
        # åªæœ‰é€‰æ‹© custom_openai æ—¶æ‰ä½¿ç”¨è‡ªå®šä¹‰ URL
        if provider == 'custom_openai':
            final_base_url = base_url if base_url else None
        else:
            final_base_url = provider_urls.get(provider)
        
        # æ—¥å¿—ï¼šæœ€ç»ˆä½¿ç”¨çš„é…ç½®
        logger.info(f"[Agentæµ‹è¯•] æœ€ç»ˆBase URL: {final_base_url if final_base_url else '(ä½¿ç”¨OpenAIé»˜è®¤)'}")
        logger.info(f"[Agentæµ‹è¯•] å¼€å§‹è°ƒç”¨ {model_name} æ¨¡å‹...")
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = OpenAI(
            api_key=api_key,
            base_url=final_base_url,
            timeout=30
        )
        
        # å‘é€æµ‹è¯•è¯·æ±‚
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "Hi"}],
            max_tokens=5
        )
        
        logger.info(f"[Agentæµ‹è¯•] âœ… è¿æ¥æˆåŠŸï¼å“åº”: {response.choices[0].message.content if response.choices else '(æ— å†…å®¹)'}")
        return jsonify({'success': True, 'message': 'è¿æ¥æˆåŠŸ'})
        
    except Exception as e:
        error_msg = str(e)
        error_type = type(e).__name__
        
        # è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
        logger.error(f"[Agentæµ‹è¯•] âŒ è¿æ¥å¤±è´¥")
        logger.error(f"[Agentæµ‹è¯•] é”™è¯¯ç±»å‹: {error_type}")
        logger.error(f"[Agentæµ‹è¯•] é”™è¯¯ä¿¡æ¯: {error_msg}")
        
        # å¦‚æœæœ‰æ›´å¤šé”™è¯¯ç»†èŠ‚
        if hasattr(e, 'response') and hasattr(e.response, 'status_code'):
            logger.error(f"[Agentæµ‹è¯•] HTTPçŠ¶æ€ç : {e.response.status_code}")
            logger.error(f"[Agentæµ‹è¯•] å“åº”ä½“: {e.response.text if hasattr(e.response, 'text') else '(æ— )'}")
        
        # è§£æå…·ä½“é”™è¯¯
        if 'authentication' in error_msg.lower() or '401' in error_msg:
            return jsonify({'success': False, 'error': 'API Key æ— æ•ˆ'}), 400
        elif '403' in error_msg or 'permission' in error_msg.lower():
            return jsonify({'success': False, 'error': f'æƒé™é”™è¯¯(403): {error_msg}'}), 400
        elif 'not found' in error_msg.lower() or '404' in error_msg:
            return jsonify({'success': False, 'error': f'æ¨¡å‹æˆ–ç«¯ç‚¹ä¸å­˜åœ¨(404): {model_name}'}), 400
        
        logger.exception("æµ‹è¯• Agent è¿æ¥å¤±è´¥")
        return jsonify({'success': False, 'error': error_msg}), 500
